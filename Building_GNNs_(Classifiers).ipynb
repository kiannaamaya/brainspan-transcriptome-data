{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPmj0mBED7We"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S8mx6uBD52m",
        "outputId": "136ff148-e16b-4789-c239-b24444124812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.3.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import joblib\n",
        "print(joblib.__version__)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MultiLabelBinarizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.nn import GATConv, global_mean_pool\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ND4o7DeD98H"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDIDL5yEDzz3"
      },
      "outputs": [],
      "source": [
        "def bin_ages(age):\n",
        "    if pd.isnull(age) or not isinstance(age, (str, float, int)):\n",
        "        return 'Unknown'\n",
        "    age_str = str(age)\n",
        "\n",
        "    if 'pcw' in age_str:\n",
        "        weeks = int(age_str.split(' ')[0])\n",
        "        if weeks >= 4 and weeks <= 7: # 4-7 pcw\n",
        "            return 'Embryonic'\n",
        "        elif weeks >= 8 and weeks <= 38: # 8-38 pcw\n",
        "            return 'Prenatal'\n",
        "    elif 'mos' in age_str or 'M' in age_str:\n",
        "        months = int(age_str.split(' ')[0])\n",
        "        if months >= 0 and months <= 19: # 0-19 months\n",
        "            return 'Infancy'\n",
        "    elif 'yrs' in age_str or 'Y' in age_str:\n",
        "        years = int(age_str.split(' ')[0])\n",
        "        if years >= 1 and years <= 11: # 1-11 years\n",
        "            return 'Childhood'\n",
        "        elif years >= 12 and years <= 19: # 12-19 years\n",
        "            return 'Adolescence'\n",
        "        elif years >= 20: # 20 years and above\n",
        "            return 'Adulthood'\n",
        "\n",
        "    # return 'Unknown' for any other cases\n",
        "    return 'Unknown'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0S00q-BGFxll"
      },
      "outputs": [],
      "source": [
        "def scale(X):\n",
        "    \"\"\"\n",
        "    Scales (standardizes) the input data.\n",
        "\n",
        "    Args:\n",
        "    - X (pd.DataFrame): Input data to be scaled.\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: Scaled (standardized) data.\n",
        "    \"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    return scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6j7Q5hYFqFz"
      },
      "outputs": [],
      "source": [
        "def create_graphs_from_correlations(X, threshold=0.8):\n",
        "    \"\"\"\n",
        "    Creates graphs based on correlation threshold.\n",
        "\n",
        "    Args:\n",
        "    - X (pd.DataFrame): Transposed DataFrame where each row is a gene and each column is a subject.\n",
        "    - threshold (float): Correlation threshold for edge creation.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of graph objects.\n",
        "    \"\"\"\n",
        "\n",
        "    correlation_matrix = X.corr().abs()  # use absolute value of correlations\n",
        "\n",
        "    print('Correlation matrix created')\n",
        "\n",
        "    edge_list = []\n",
        "    for i in range(correlation_matrix.shape[0]):\n",
        "        for j in range(i + 1, correlation_matrix.shape[1]):\n",
        "            if correlation_matrix.iloc[i, j] >= threshold:\n",
        "                edge_list.append((i, j))\n",
        "    print('Edge list created')\n",
        "\n",
        "    edge_index_tensor = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "    print('Converted to tensor')\n",
        "\n",
        "    graphs = []\n",
        "    for col in X.columns:\n",
        "        node_features = torch.tensor(X[col].values, dtype=torch.float).view(-1, 1)\n",
        "        graph = Data(x=node_features, edge_index=edge_index_tensor)\n",
        "        graphs.append(graph)\n",
        "    print('Graphs created')\n",
        "    return graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFWCrSVkED53"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data_path, file_type='csv'):\n",
        "    \"\"\"\n",
        "    Preprocess the data and return the train-test split.\n",
        "\n",
        "    Args:\n",
        "    - data_path (str): Path to the data file.\n",
        "    - file_type (str): File format ('csv', 'excel', 'txt').\n",
        "\n",
        "    Returns:\n",
        "    - X_train, X_test, y_train, y_test: Train-test split of the preprocessed data.\n",
        "    \"\"\"\n",
        "    subset_rows = 20\n",
        "\n",
        "\n",
        "    if 'methylation' in data_path.lower() and file_type == 'csv':\n",
        "        chunk_size = 5\n",
        "        chunks = pd.read_csv(data_path, chunksize=chunk_size)\n",
        "\n",
        "        X_list = []\n",
        "        y_list = []\n",
        "        for chunk in chunks:\n",
        "            chunk = chunk.head(subset_rows)\n",
        "            X_chunk, y_chunk = preprocess_chunk(chunk, 'Methylation')\n",
        "\n",
        "            X_list.append(X_chunk)\n",
        "            y_list.append(y_chunk)\n",
        "\n",
        "        X_data = pd.concat(X_list, axis=0)\n",
        "        y_data = pd.concat(y_list, axis=0)\n",
        "\n",
        "        data = pd.concat([X_data, y_data], axis=1)\n",
        "        print(\"Columns after processing all chunks:\", data.columns)\n",
        "\n",
        "    else:\n",
        "        if file_type == 'csv':\n",
        "            data = pd.read_csv(data_path, index_col=0)\n",
        "        elif file_type == 'excel':\n",
        "            data = pd.read_excel(data_path, index_col=0)\n",
        "        elif file_type == 'txt':\n",
        "            data = pd.read_csv(data_path, sep='\\t', index_col=0)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file type\")\n",
        "        print(\"Columns after determining data type:\", data.columns)\n",
        "\n",
        "    if 'rnaseq' in data_path.lower():\n",
        "        data_type = 'RNA-Seq'\n",
        "    elif 'methylation' in data_path.lower():\n",
        "        data_type = 'Methylation'\n",
        "    elif 'microrna' in data_path.lower():\n",
        "        data_type = 'MicroRNA'\n",
        "    else:\n",
        "        raise ValueError(\"Unknown data type\")\n",
        "\n",
        "\n",
        "\n",
        "    data.reset_index(drop=True, inplace=True)\n",
        "    data['age_group'] = data['age'].apply(bin_ages)\n",
        "\n",
        "    print(data.columns)\n",
        "\n",
        "    if data_type == 'RNA-Seq':\n",
        "        label_map = {'Prenatal': 0, 'Infancy': 0, 'Childhood': 1, 'Adolescence': 2, 'Adulthood': 3}\n",
        "    elif data_type == 'MicroRNA':\n",
        "        label_map = {'Infancy': 0, 'Childhood': 1, 'Adolescence': 2, 'Adulthood': 3}\n",
        "    else:\n",
        "        label_map = {'Embryonic': 0, 'Prenatal': 1, 'Infancy': 2, 'Childhood': 3, 'Adolescence': 4, 'Adulthood': 5}\n",
        "\n",
        "    data['age_group'] = data['age_group'].map(label_map)\n",
        "\n",
        "    if data_type == 'RNA-Seq':\n",
        "        data_numeric = data.drop(['age', 'age_group'], axis=1)\n",
        "        one_percent_of_samples = data_numeric.shape[1] * 0.01\n",
        "        mask = data_numeric.gt(1).sum(axis=1) >= one_percent_of_samples\n",
        "        filtered_data = data[mask]\n",
        "\n",
        "        y = torch.tensor(filtered_data['age_group'].values, dtype=torch.float)\n",
        "        X = filtered_data.drop(['age', 'age_group'], axis=1).transpose()\n",
        "\n",
        "    elif data_type == 'Methylation':\n",
        "        relevant_columns = [col for col in data.columns if col.startswith(('cg', 'rs', 'ch'))] + ['age_group']\n",
        "        filtered_data = data[relevant_columns]\n",
        "        X = filtered_data.drop(['age_group'], axis=1).transpose()\n",
        "        print(X.head(5))\n",
        "        y = filtered_data['age_group']\n",
        "\n",
        "    elif data_type == 'MicroRNA':\n",
        "        relevant_columns = [col for col in data.columns if col.startswith('hsa')] + ['age', 'age_group']\n",
        "        filtered_data = data[relevant_columns]\n",
        "\n",
        "        y = torch.tensor(filtered_data['age_group'].values, dtype=torch.float)\n",
        "        X = filtered_data.drop(['age', 'age_group'], axis=1).transpose()\n",
        "\n",
        "\n",
        "    print(np.unique(y))\n",
        "    # print(\"About to shuffle\")\n",
        "    print(\"X shape:\", X.shape)\n",
        "    print(\"y shape:\", y.shape)\n",
        "\n",
        "    # X, y = shuffle(X, y, random_state=0)\n",
        "    X_scaled = scale(X, with_mean=False)\n",
        "\n",
        "    X_scaled_df = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\n",
        "\n",
        "    graphs = create_graphs_from_correlations(pd.DataFrame(X_scaled, index=X.index), threshold=0.8)\n",
        "\n",
        "    graphs_train, graphs_test, y_train, y_test = train_test_split(graphs, y, test_size=0.2, random_state=42)\n",
        "    print(\"The data is split\")\n",
        "\n",
        "    return graphs_train, graphs_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOj4uzjXEOtB"
      },
      "outputs": [],
      "source": [
        "def save_data_splits(X_train, X_test, y_train, y_test, output_dir):\n",
        "    \"\"\"\n",
        "    Saves the train-test data splits to the specified directory using joblib.\n",
        "\n",
        "    Args:\n",
        "    - X_train (pd.DataFrame or np.ndarray): Training data features.\n",
        "    - X_test (pd.DataFrame or np.ndarray): Testing data features.\n",
        "    - y_train (pd.Series or np.ndarray): Training data labels.\n",
        "    - y_test (pd.Series or np.ndarray): Testing data labels.\n",
        "    - output_dir (str): Directory path where the data splits will be saved.\n",
        "\n",
        "    Note:\n",
        "    - If the output directory does not exist, it will be created.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    joblib.dump(X_train, os.path.join(output_dir, 'X_train_classifiers.pkl'))\n",
        "    joblib.dump(X_test, os.path.join(output_dir, 'X_test_classifiers.pkl'))\n",
        "    joblib.dump(y_train, os.path.join(output_dir, 'y_train_classifiers.pkl'))\n",
        "    joblib.dump(y_test, os.path.join(output_dir, 'y_test_classifiers.pkl'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV0orxeqHX_i"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(graphs, ages, batch_size=32):\n",
        "    graph_data_list = []\n",
        "    for i, graph in enumerate(graphs):\n",
        "        age_label = torch.tensor([ages[i]], dtype=torch.float)\n",
        "        graph_data_list.append(Data(x=graph.x, edge_index=graph.edge_index, y=age_label))\n",
        "\n",
        "    return DataLoader(graph_data_list, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37EzxEw6IDkd"
      },
      "outputs": [],
      "source": [
        "def train_evaluate_gat_model(train_loader, test_loader, num_classes, epochs=100):\n",
        "    \"\"\"\n",
        "    Trains and evaluates a GAT model.\n",
        "\n",
        "    Args:\n",
        "    - train_loader (DataLoader): DataLoader for training data.\n",
        "    - test_loader (DataLoader): DataLoader for test data.\n",
        "    - num_classes (int): Number of classes for classification.\n",
        "    - epochs (int): Number of training epochs.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained GAT model.\n",
        "    - avg_test_loss: Average loss on the test set.\n",
        "    - test_accuracy: Accuracy on the test set.\n",
        "    \"\"\"\n",
        "    class GATNetClassifier(torch.nn.Module):\n",
        "      def __init__(self, num_classes):\n",
        "          super(GATNetClassifier, self).__init__()\n",
        "          self.conv1 = GATConv(1, 8, heads=8, dropout=0.6)  # accepts 1 feature per node\n",
        "          self.conv2 = GATConv(8 * 8, 16, heads=1, dropout=0.6)\n",
        "          self.fc = torch.nn.Linear(16, num_classes)  # output layer for classification\n",
        "\n",
        "      def forward(self, data):\n",
        "          x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "          x = F.dropout(x, p=0.6, training=self.training)\n",
        "          x = F.elu(self.conv1(x, edge_index))\n",
        "          x = F.dropout(x, p=0.6, training=self.training)\n",
        "          x = F.elu(self.conv2(x, edge_index))\n",
        "\n",
        "          x = global_mean_pool(x, batch)  # pooling to get graph-level representation\n",
        "          x = self.fc(x)\n",
        "          return F.log_softmax(x, dim=1)  # use log_softmax for classification\n",
        "\n",
        "    model = GATNetClassifier(num_classes)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch)\n",
        "            loss = criterion(output, batch.y.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            output = model(batch)\n",
        "            loss = criterion(output, batch.y.long())\n",
        "            total_test_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += batch.y.size(0)\n",
        "            correct += (predicted == batch.y.long()).sum().item()\n",
        "\n",
        "    avg_test_loss = total_test_loss / len(test_loader)\n",
        "    test_accuracy = 100 * correct / total\n",
        "\n",
        "    return model, avg_test_loss, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5p1GQWXEVI0"
      },
      "outputs": [],
      "source": [
        "def main(data_paths):\n",
        "\n",
        "    for data_path in data_paths:\n",
        "\n",
        "        data_type = os.path.basename(data_path).split('_')[0]\n",
        "\n",
        "        graphs_train, graphs_test, y_train, y_test = preprocess_data(data_path)\n",
        "        save_data_splits(graphs_train, graphs_test, y_train, y_test, os.path.join('baseline_cnn_outputs', data_type))\n",
        "\n",
        "        train_loader = create_data_loader(graphs_train, y_train)\n",
        "        print('Train loader created')\n",
        "        test_loader = create_data_loader(graphs_test, y_test)\n",
        "        print('Test loader created')\n",
        "\n",
        "        num_classes = len(set(y_train.numpy()))\n",
        "        model, avg_test_loss, test_accuracy = train_evaluate_gat_model(train_loader, test_loader, num_classes, epochs=10)\n",
        "\n",
        "        print(f\"GAT Model - Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vQoeYhPEbol",
        "outputId": "51b769f6-0ed3-47a9-e3f0-07e0d8ec0c55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns after determining data type: Index(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
            "       ...\n",
            "       '22319', '22320', '22321', '22322', '22323', '22324', '22325', '22326',\n",
            "       '22327', 'age'],\n",
            "      dtype='object', length=22328)\n",
            "Index(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
            "       ...\n",
            "       '22320', '22321', '22322', '22323', '22324', '22325', '22326', '22327',\n",
            "       'age', 'age_group'],\n",
            "      dtype='object', length=22329)\n",
            "[0. 1. 2. 3.]\n",
            "About to shuffle\n",
            "X shape: (22327, 578)\n",
            "y shape: torch.Size([578])\n",
            "Correlation matrix created\n",
            "Edge list created\n",
            "Converted to tensor\n",
            "Graphs created\n",
            "The data is split\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loader created\n",
            "Test loader created\n",
            "GAT Model - Test Loss: 1.0557, Test Accuracy: 56.03%\n",
            "Columns after determining data type: Index(['hsa-miR-26a-5p', 'hsa-miR-181a-5p', 'hsa-miR-143-3p', 'hsa-let-7a-5p',\n",
            "       'hsa-miR-9-5p', 'hsa-miR-3182', 'hsa-miR-99b-5p', 'hsa-miR-30a-5p',\n",
            "       'hsa-miR-27b-3p', 'hsa-miR-191-5p',\n",
            "       ...\n",
            "       'hsa-miR-4653-5p', 'hsa-miR-4264', 'hsa-miR-3119', 'hsa-miR-4330',\n",
            "       'hsa-miR-4318', 'hsa-miR-4279', 'hsa-miR-3689f', 'hsa-miR-4291',\n",
            "       'donor_name', 'age'],\n",
            "      dtype='object', length=1863)\n",
            "Index(['hsa-miR-26a-5p', 'hsa-miR-181a-5p', 'hsa-miR-143-3p', 'hsa-let-7a-5p',\n",
            "       'hsa-miR-9-5p', 'hsa-miR-3182', 'hsa-miR-99b-5p', 'hsa-miR-30a-5p',\n",
            "       'hsa-miR-27b-3p', 'hsa-miR-191-5p',\n",
            "       ...\n",
            "       'hsa-miR-4264', 'hsa-miR-3119', 'hsa-miR-4330', 'hsa-miR-4318',\n",
            "       'hsa-miR-4279', 'hsa-miR-3689f', 'hsa-miR-4291', 'donor_name', 'age',\n",
            "       'age_group'],\n",
            "      dtype='object', length=1864)\n",
            "[0. 1. 2. 3.]\n",
            "About to shuffle\n",
            "X shape: (1861, 216)\n",
            "y shape: torch.Size([216])\n",
            "Correlation matrix created\n",
            "Edge list created\n",
            "Converted to tensor\n",
            "Graphs created\n",
            "The data is split\n",
            "Train loader created\n",
            "Test loader created\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GAT Model - Test Loss: 1.2476, Test Accuracy: 50.00%\n"
          ]
        }
      ],
      "source": [
        "data_paths = [#'methylation_1.csv',\n",
        "              '/content/drive/MyDrive/rnaseq/rnaseq_1.csv',\n",
        "              '/content/drive/MyDrive/microRNA/microRNA_1.csv'\n",
        "              ]\n",
        "main(data_paths)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}